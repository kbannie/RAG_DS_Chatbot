import streamlit as st

from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from langchain import hub
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_pinecone import PineconeVectorStore

st.set_page_config(page_title="ë•ì„±ì—¬ëŒ€ ê·œì • ì±—ë´‡", page_icon="ğŸ¤–")

st.title("ğŸ¤– ë•ì„±ì—¬ëŒ€ ê·œì • ì±—ë´‡")
st.caption("ë•ì„±ì—¬ëŒ€ ê·œì •ì— ê´€ë ¨ëœ ëª¨ë“ ê²ƒì„ ë‹µí•´ë“œë¦½ë‹ˆë‹¤!")

load_dotenv()

if 'message_list' not in st.session_state:
    st.session_state.message_list = []

for message in st.session_state.message_list:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def get_ai_message(user_message):

    embedding = OpenAIEmbeddings(model='text-embedding-3-large')
    index_name = 'ds-chatbot'
    database = PineconeVectorStore.from_existing_index(index_name=index_name, embedding=embedding)

    llm = ChatOpenAI(model='gpt-4o')
    prompt = hub.pull("rlm/rag-prompt")
    retriever = database.as_retriever(search_kwargs={'k': 4})

    qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever, chain_type_kwargs={"prompt": prompt})

    ds_chain = qa_chain
    
    ai_message = ds_chain.invoke({"query": user_message})

    return ai_message['result']


if user_question := st.chat_input(placeholder="ë•ì„±ì—¬ëŒ€ ê·œì •ì— ê´€ë ¨ëœ ê¶ê¸ˆí•œ ë‚´ìš©ë“¤ì„ ë§ì”€í•´ì£¼ì„¸ìš”!"):
    with st.chat_message("user"):
        st.write(user_question)
    st.session_state.message_list.append({"role": "user", "content": user_question})

    with st.spinner("ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤"):
        ai_message = get_ai_message(user_question)
        with st.chat_message("ai"):
            st.write(ai_message)
            st.session_state.message_list.append({"role": "ai", "content": ai_message})